{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud and boost specific words?\n",
    "# statsmodels, ordinary least squares, pvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../data/S1_8Sharktankpitchesdeals.csv\"\n",
    "data = pd.read_csv(fname)\n",
    "#valuation is a component of the amoutn asked for and the stake offered therefor doesn't need to be included in the machine learning\n",
    "data = data.drop(columns=[\"Valuation\"])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(columns=data.columns)\n",
    "offset = 0;\n",
    "\n",
    "for i, j in data.iterrows():\n",
    "    data2 = data2.append(data.loc[i].to_dict(), ignore_index=True)\n",
    "    \n",
    "    if not pd.isna(data[\"Deal_Shark 2\"][i]):\n",
    "        data2 = data2.append(data.loc[i].to_dict(), ignore_index=True)\n",
    "        offset+=1\n",
    "        data2[\"Deal_Shark 1\"][offset+i] = data.loc[i][\"Deal_Shark 2\"]\n",
    "    if not pd.isna(data[\"Deal_Shark 3\"][i]):\n",
    "        data2 = data2.append(data.loc[i].to_dict(), ignore_index=True)\n",
    "        offset+=1\n",
    "        data2[\"Deal_Shark 1\"][offset+i] = data.loc[i][\"Deal_Shark 3\"]\n",
    "    if not pd.isna(data[\"Deal_Shark 4\"][i]):\n",
    "        data2 = data2.append(data.loc[i].to_dict(), ignore_index=True)\n",
    "        offset+=1\n",
    "        data2[\"Deal_Shark 1\"][offset+i] = data.loc[i][\"Deal_Shark 4\"]\n",
    "    if not pd.isna(data[\"Deal_Shark 5\"][i]):\n",
    "        data2 = data2.append(data.loc[i].to_dict(), ignore_index=True)\n",
    "        offset+=1\n",
    "        data2[\"Deal_Shark 1\"][offset+i] = data.loc[i][\"Deal_Shark 5\"]\n",
    "\n",
    "\n",
    "data2 = data2.drop(columns=[\"Deal_Shark 2\",\"Deal_Shark 3\",\"Deal_Shark 4\",\"Deal_Shark 5\"])\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.infer_objects()\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.infer_objects()\n",
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model output\n",
    "\n",
    "# \"Deal_Status\"\n",
    "# \"Deal_Shark 1\"\n",
    "\n",
    "# \"Kevin O'Leary\"\n",
    "# \"Barbara Corcoran\"\n",
    "# \"Robert Herjavec\"\n",
    "# \"Daymond John\"\n",
    "# \"Lori Greiner\"\n",
    "# \"Mark Cuban\"\n",
    "# \"Others\"\n",
    "shark_to_model = \"Deal_Status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where shark_to_model is Nan\n",
    "data=data.dropna(subset=[shark_to_model])\n",
    "data2=data2.dropna(subset=[shark_to_model])\n",
    "# .reset_index(drop=True)\n",
    "data = data.reset_index(drop=True)\n",
    "data2 = data2.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer from class activity 22-1-4\n",
    "stopwords = stopwords.words( 'english' ) + list(punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming\n",
    "punc_list = list(punctuation)\n",
    "def special_remove(word):\n",
    "    if len(word)>2:\n",
    "        return False\n",
    "    for c in word:\n",
    "        if c in punc_list:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# custom function that overrides default token generation\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [stemmer.stem(w) for w in words if w not in stopwords+[\"...\"]]\n",
    "    # further remove words with a special char\n",
    "    words = [w for w in words if not special_remove(w)]\n",
    "    return words\n",
    "\n",
    "activity_vectorizer = TfidfVectorizer(ngram_range=(1,2)\n",
    "                            ,tokenizer=custom_tokenizer \n",
    "                            ,stop_words='english'\n",
    "                            # ,min_df=2\n",
    "                            ,strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "activity_vectors = activity_vectorizer.fit_transform(data[\"Pitched_Business_Desc\"])\n",
    "activity_df = pd.DataFrame(activity_vectors.toarray(), columns=activity_vectorizer.get_feature_names())\n",
    "activity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_vectors2 = activity_vectorizer.fit_transform(data2[\"Pitched_Business_Desc\"])\n",
    "activity_df2 = pd.DataFrame(activity_vectors2.toarray(), columns=activity_vectorizer.get_feature_names())\n",
    "activity_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "\n",
    "# To include Pitch, comment in next line, and comment out the following line\n",
    "# To exclude Pitch, comment out next line, and comment in the following line\n",
    "\n",
    "selected_features = activity_df\n",
    "selected_features2 = activity_df2\n",
    "# selected_features = pd.DataFrame([])\n",
    "\n",
    "selected_features[\"Gender\"]=data[\"Gender\"]\n",
    "selected_features[\"Category\"]=data[\"Category\"]\n",
    "selected_features[\"Amount_Asked_For\"]=data[\"Amount_Asked_For\"]\n",
    "selected_features[\"Exchange_For_Stake\"]=data[\"Exchange_For_Stake\"]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features2[\"Gender\"]=data2[\"Gender\"]\n",
    "selected_features2[\"Category\"]=data2[\"Category\"]\n",
    "selected_features2[\"Amount_Asked_For\"]=data2[\"Amount_Asked_For\"]\n",
    "selected_features2[\"Exchange_For_Stake\"]=data2[\"Exchange_For_Stake\"]\n",
    "selected_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = pd.get_dummies(selected_features)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features2 = pd.get_dummies(selected_features2)\n",
    "selected_features2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(selected_features, data[[shark_to_model]].values.ravel())\n",
    "X = X_smote\n",
    "y = y_smote\n",
    "\n",
    "# X = selected_features\n",
    "# y = data[[shark_to_model]].values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultinomialNB()\n",
    "\n",
    "X_smote2, y_smote2 = SMOTE().fit_sample(selected_features2, data2[[shark_to_model]].values.ravel())\n",
    "X2 = X_smote2\n",
    "y2 = y_smote2\n",
    "\n",
    "# X2 = selected_features2\n",
    "# y2 = data2[[shark_to_model]].values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "print(X2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=6)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfpvalues = pd.DataFrame(fit.pvalues_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores,dfpvalues],axis=1)\n",
    "featureScores.columns = ['Specs','Score','PValue']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=6)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfpvalues = pd.DataFrame(fit.pvalues_)\n",
    "dfcolumns = pd.DataFrame(X2.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores,dfpvalues],axis=1)\n",
    "featureScores.columns = ['Specs','Score','PValue']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"actual\": y_test.reshape(-1), \"prediction\": y_pred.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train2, y_train2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"actual\": y_test2.reshape(-1), \"prediction\": y_pred2.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test2,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "scores = ['precision', 'recall']\n",
    "model3 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        model3, param_grid, scoring='%s_macro' % score,verbose=3)\n",
    "    clf.fit(X_train2, y_train2)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test2, clf.predict(X_test2)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression # Regression classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier\n",
    "from sklearn import svm # Support Vector Machine\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Random Forest and Gradient Boosting Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier \n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix # Some metrics to check the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for each algorithm - these are tunable to achieve max accuracy\n",
    "\n",
    "Classifiers = {'LR':LogisticRegression(random_state=10,C=5,max_iter=300, solver='lbfgs')\n",
    "               ,'DTC':DecisionTreeClassifier(random_state=10,min_samples_leaf=2)\n",
    "               ,'RF':RandomForestClassifier(random_state=10,n_estimators=100,n_jobs=-1)\n",
    "               # ,'GBC':GradientBoostingClassifier(random_state=10,n_estimators=400,learning_rate=0.2)\n",
    "               ,'SGD':SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "               # ,'SVM':svm.SVC(kernel='linear', C=0.1)\n",
    "               ,'NB':MultinomialNB(alpha=.0001)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline so you can reuse the code\n",
    "\n",
    "def ML_Pipeline(clf_name):\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_test)\n",
    "    Accuracy = accuracy_score(y_test,y_pred)\n",
    "    Precision = metrics.precision_score(y_test,y_pred)\n",
    "    Recall = recall_score(y_test,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20) \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline so you can reuse the code\n",
    "\n",
    "def ML_Pipeline2(clf_name):\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train2, y_train2.ravel())\n",
    "    y_pred = clf.predict(X_test2)\n",
    "    F1 = f1_score(y_test2, y_pred, zero_division=1)\n",
    "    Accuracy = accuracy_score(y_test2,y_pred)\n",
    "    Precision = metrics.precision_score(y_test2,y_pred)\n",
    "    Recall = recall_score(y_test2,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test2,y_pred)\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('F1 = '+str(F1))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20) \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = []\n",
    "for clf_name in Classifiers.keys():\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_test)\n",
    "    F1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "    Accuracy = accuracy_score(y_test,y_pred)\n",
    "    Precision = metrics.precision_score(y_test,y_pred)\n",
    "    Recall = recall_score(y_test,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    clf_dict.append({'Classifier': clf_name, 'F1': F1, 'Accuracy': Accuracy, 'Precision': Precision, 'Recall': Recall})\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict2 = []\n",
    "for clf_name in Classifiers.keys():\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train2, y_train2.ravel())\n",
    "    y_pred = clf.predict(X_test2)\n",
    "    F1 = f1_score(y_test2, y_pred, zero_division=1)\n",
    "    Accuracy = accuracy_score(y_test2,y_pred)\n",
    "    Precision = metrics.precision_score(y_test2,y_pred)\n",
    "    Recall = recall_score(y_test2,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test2,y_pred)\n",
    "    clf_dict2.append({'Classifier': clf_name, 'F1': F1, 'Accuracy': Accuracy, 'Precision': Precision, 'Recall': Recall})\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('F1 = '+str(F1))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.DataFrame(clf_dict2)\n",
    "clf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df['Accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = clf_df.loc[clf_df['Accuracy'].idxmax(),'Classifier']\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ML_Pipeline2(best_clf)\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, f\"{shark_to_model}_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(selected_features.columns,open(f\"{shark_to_model}_vocab.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
