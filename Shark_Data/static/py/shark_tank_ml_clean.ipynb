{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud and boost specific words?\n",
    "# statsmodels, ordinary least squares, pvalue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../data/S1_8Sharktankpitchesdeals.csv\"\n",
    "data = pd.read_csv(fname)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model output\n",
    "\n",
    "# \"Deal_Status\"\n",
    "# \"Kevin O'Leary\"\n",
    "# \"Barbara Corcoran\"\n",
    "# \"Robert Herjavec\"\n",
    "# \"Daymond John\"\n",
    "# \"Kevin Harrington\"\n",
    "# \"Mark Cuban\"\n",
    "# \"Others\"\n",
    "shark_to_model = \"Mark Cuban\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where shark_to_model is Nan\n",
    "data=data.dropna(subset=[shark_to_model])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer from class activity 22-1-4\n",
    "stopwords = stopwords.words( 'english' ) + list(punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming\n",
    "punc_list = list(punctuation)\n",
    "def special_remove(word):\n",
    "    if len(word)>2:\n",
    "        return False\n",
    "    for c in word:\n",
    "        if c in punc_list:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# custom function that overrides default token generation\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [stemmer.stem(w) for w in words if w not in stopwords+[\"...\"]]\n",
    "    # further remove words with a special char\n",
    "    words = [w for w in words if not special_remove(w)]\n",
    "    return words\n",
    "\n",
    "activity_vectorizer = TfidfVectorizer(ngram_range=(1,2)\n",
    "                            ,tokenizer=custom_tokenizer \n",
    "                            ,stop_words='english'\n",
    "                            # ,min_df=2\n",
    "                            ,strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "activity_vectors = activity_vectorizer.fit_transform(data[\"Pitched_Business_Desc\"])\n",
    "activity_df = pd.DataFrame(activity_vectors.toarray(), columns=activity_vectorizer.get_feature_names())\n",
    "activity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "\n",
    "# To include Pitch, comment in next line, and comment out the following line\n",
    "# To exclude Pitch, comment out next line, and comment in the following line\n",
    "selected_features = activity_df\n",
    "# selected_features = pd.DataFrame([])\n",
    "selected_features[\"Gender\"]=data[\"Gender\"]\n",
    "selected_features[\"Category\"]=data[\"Category\"]\n",
    "selected_features[\"Amount_Asked_For\"]=data[\"Amount_Asked_For\"]\n",
    "selected_features[\"Exchange_For_Stake\"]=data[\"Exchange_For_Stake\"]\n",
    "selected_features[\"Valuation\"]=data[\"Valuation\"]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = pd.get_dummies(selected_features)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "X = selected_features\n",
    "y = data[[shark_to_model]].values.ravel()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2, f_classif\n",
    "# #apply SelectKBest class to extract top 10 best features\n",
    "# bestfeatures = SelectKBest(score_func=f_classif, k=6)\n",
    "# fit = bestfeatures.fit(X,y)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfpvalues = pd.DataFrame(fit.pvalues_)\n",
    "# dfcolumns = pd.DataFrame(X.columns)\n",
    "# #concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns,dfscores,dfpvalues],axis=1)\n",
    "# featureScores.columns = ['Specs','Score','PValue']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import chi2, f_classif\n",
    "# #apply SelectKBest class to extract top 10 best features\n",
    "# bestfeatures = SelectKBest(score_func=chi2, k=6)\n",
    "# fit = bestfeatures.fit(X,y)\n",
    "# dfscores = pd.DataFrame(fit.scores_)\n",
    "# dfpvalues = pd.DataFrame(fit.pvalues_)\n",
    "# dfcolumns = pd.DataFrame(X.columns)\n",
    "# #concat two dataframes for better visualization \n",
    "# featureScores = pd.concat([dfcolumns,dfscores,dfpvalues],axis=1)\n",
    "# featureScores.columns = ['Specs','Score','PValue']  #naming the dataframe columns\n",
    "# print(featureScores.nlargest(20,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"actual\": y_test.reshape(-1), \"prediction\": y_pred.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression # Regression classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier\n",
    "from sklearn import svm # Support Vector Machine\n",
    "from sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Random Forest and Gradient Boosting Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier \n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix # Some metrics to check the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for each algorithm - these are tunable to achieve max accuracy\n",
    "\n",
    "Classifiers = {'LR':LogisticRegression(random_state=10,C=5,max_iter=300, solver='lbfgs')\n",
    "               ,'DTC':DecisionTreeClassifier(random_state=10,min_samples_leaf=2)\n",
    "               ,'RF':RandomForestClassifier(random_state=10,n_estimators=100,n_jobs=-1)\n",
    "               ,'GBC':GradientBoostingClassifier(random_state=10,n_estimators=400,learning_rate=0.2)\n",
    "               ,'SGD':SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "            #    ,'SVM':svm.SVC(kernel='linear', C=0.1)\n",
    "               ,'NB':MultinomialNB(alpha=.05)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline so you can reuse the code\n",
    "\n",
    "def ML_Pipeline(clf_name):\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_test)\n",
    "    Accuracy = accuracy_score(y_test,y_pred)\n",
    "    Precision = metrics.precision_score(y_test,y_pred)\n",
    "    Recall = recall_score(y_test,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20) \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = []\n",
    "for clf_name in Classifiers.keys():\n",
    "    clf = Classifiers[clf_name]\n",
    "    fit = clf.fit(X_train, y_train.ravel())\n",
    "    y_pred = clf.predict(X_test)\n",
    "    Accuracy = accuracy_score(y_test,y_pred)\n",
    "    Precision = metrics.precision_score(y_test,y_pred)\n",
    "    Recall = recall_score(y_test,y_pred)\n",
    "    Confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    clf_dict.append({'Classifier': clf_name, 'Accuracy': Accuracy, 'Precision': Precision, 'Recall': Recall})\n",
    "    print('Classifier = '+(clf_name))\n",
    "    print('Accuracy = '+str(Accuracy))\n",
    "    print('Precision = '+str(Precision))\n",
    "    print('Recall = '+str(Recall))\n",
    "    print(Confusion_matrix)\n",
    "    print('==='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.DataFrame(clf_dict)\n",
    "clf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df['Accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = clf_df.loc[clf_df['Accuracy'].idxmax(),'Classifier']\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ML_Pipeline(best_clf)\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_model, f\"{shark_to_model}_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(selected_features.columns,open(f\"{shark_to_model}_vocab.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Conda [PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
